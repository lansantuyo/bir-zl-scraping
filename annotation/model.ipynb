{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torchcrf import CRF"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "KEYWORDS_FOR_STARTS_WITH = [\"Province\", \"City\", \"Municipality\", \"Barangay\", \"Zone\", \"Street\"]\n",
    "ZV_PATTERN_REGEX = re.compile(\n",
    "    r\"\\d+(?:ST|ND|RD|TH)\\s+(?:REVISION|Rev)(?:.*Z\\.?V\\.?.*SQ.*M\\.?)?|\"\n",
    "    r\"(?:\\d+(?:ST|ND|RD|TH)\\s+REVISION|Rev\\s+ZV\\s+/?.*SQ\\.?\\s*M\\.?)|\"\n",
    "    r\"(?:Z|2)\\.?V\\.?.*SQ.*M\\.?|FINAL\",\n",
    "    re.IGNORECASE\n",
    ")"
   ],
   "id": "20d65885cc432b07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "VOCAB_SIZE = 20000\n",
    "EMBEDDING_DIM = 128\n",
    "TEXT_SEQ_OUTPUT_LEN = 50\n",
    "LSTM_HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\""
   ],
   "id": "e819cd313aefcc0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Engineering",
   "id": "24ed5a82f5c91260"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_numeric_str(s: str) -> bool:\n",
    "    if not s: return False\n",
    "    s = re.sub(r'[â‚±,]', '', s)\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_single_row_features(raw_cells_list: list[str]) -> dict:\n",
    "    scalar_features_dict = {}\n",
    "    non_whitespace_cells_texts = [str(cell_text) for cell_text in raw_cells_list if\n",
    "                                  cell_text is not None and str(cell_text).strip() != \"\"]\n",
    "    concatenated_text = \" \".join(non_whitespace_cells_texts)\n",
    "    \n",
    "    scalar_features_dict['num_cells_in_row'] = float(len(raw_cells_list))\n",
    "    first_non_null_idx = -1\n",
    "    for i, cell_text in enumerate(raw_cells_list):\n",
    "        if cell_text is not None and str(cell_text).strip() != \"\":\n",
    "            first_non_null_idx = i\n",
    "            break\n",
    "    scalar_features_dict['first_non_null_column'] = float(first_non_null_idx)\n",
    "    last_non_null_idx = -1\n",
    "    for i in range(len(raw_cells_list) - 1, -1, -1):\n",
    "        cell_text = raw_cells_list[i]\n",
    "        if cell_text is not None and str(cell_text).strip() != \"\":\n",
    "            last_non_null_idx = i\n",
    "            break\n",
    "    scalar_features_dict['last_non_null_column'] = float(last_non_null_idx)\n",
    "    scalar_features_dict['num_non_empty_cells'] = float(len(non_whitespace_cells_texts))\n",
    "    if scalar_features_dict['num_cells_in_row'] > 0:\n",
    "        scalar_features_dict['ratio_non_empty_cells'] = scalar_features_dict['num_non_empty_cells'] / \\\n",
    "                                                        scalar_features_dict['num_cells_in_row']\n",
    "    else:\n",
    "        scalar_features_dict['ratio_non_empty_cells'] = 0.0\n",
    "    numeric_cell_count = sum(1 for text in non_whitespace_cells_texts if is_numeric_str(text))\n",
    "    scalar_features_dict['num_numeric_cells'] = float(numeric_cell_count)\n",
    "    if scalar_features_dict['num_non_empty_cells'] > 0:\n",
    "        scalar_features_dict['ratio_numeric_cells'] = numeric_cell_count / scalar_features_dict['num_non_empty_cells']\n",
    "    else:\n",
    "        scalar_features_dict['ratio_numeric_cells'] = 0.0\n",
    "    scalar_features_dict['is_row_empty_or_whitespace_only'] = 1.0 if scalar_features_dict[\n",
    "                                                                         'num_non_empty_cells'] == 0 else 0.0\n",
    "    scalar_features_dict['is_all_caps'] = 1.0 if concatenated_text.isupper() else 0.0\n",
    "    first_cell_raw = str(raw_cells_list[0]) if raw_cells_list and raw_cells_list[0] is not None else \"\"\n",
    "    for keyword in KEYWORDS_FOR_STARTS_WITH:\n",
    "        scalar_features_dict[f'starts_with_keyword_{keyword.lower().replace(\" \", \"_\")}'] = \\\n",
    "            1.0 if first_cell_raw.lower().startswith(keyword.lower()) else 0.0\n",
    "    raw_concatenated_for_zv = \" \".join(str(cell) for cell in raw_cells_list if cell is not None)\n",
    "    scalar_features_dict['contains_keyword_zv'] = 1.0 if bool(ZV_PATTERN_REGEX.search(raw_concatenated_for_zv)) else 0.0\n",
    "    feature_order = [\n",
    "                        'num_cells_in_row', 'first_non_null_column', 'last_non_null_column',\n",
    "                        'num_non_empty_cells', 'ratio_non_empty_cells', 'num_numeric_cells',\n",
    "                        'ratio_numeric_cells', 'is_row_empty_or_whitespace_only', 'contains_keyword_zv',\n",
    "                        'is_all_caps'\n",
    "                    ] + [f'starts_with_keyword_{kw.lower().replace(\" \", \"_\")}' for kw in KEYWORDS_FOR_STARTS_WITH]\n",
    "    numeric_features_list = [scalar_features_dict.get(fname, 0.0) for fname in feature_order]\n",
    "    return {\n",
    "        'concatenated_text_clean': concatenated_text,\n",
    "        'numeric_features': np.array(numeric_features_list, dtype=np.float32)\n",
    "    }"
   ],
   "id": "a5c96de48353059a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Vectorization",
   "id": "cb46ff62ef4e0bda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TODO\n",
    "- out of vocab\n",
    "- save to memory make it persist for reuse\n",
    "\n",
    "implement smth like this so we can tokenize important abbr like zv if needed\n",
    "```\n",
    "def tokenize(self, text):\n",
    "    # Preserve important spreadsheet tokens like \"Z.V.\"\n",
    "    preserved_tokens = re.findall(r'[A-Z]\\.?[A-Z]\\.?(?:/[A-Z]\\.?[A-Z]\\.?)?', text)\n",
    "    # Replace them temporarily\n",
    "    for i, token in enumerate(preserved_tokens):\n",
    "        text = text.replace(token, f\"__PRESERVED_{i}__\")\n",
    "    \n",
    "    # Standard tokenization\n",
    "    tokens = text.lower().split()\n",
    "    \n",
    "    # Restore preserved tokens\n",
    "    for i, token in enumerate(preserved_tokens):\n",
    "        for j, t in enumerate(tokens):\n",
    "            if t == f\"__preserved_{i}__\":\n",
    "                tokens[j] = token\n",
    "    \n",
    "    return tokens\n",
    "```"
   ],
   "id": "9fe9950827ca65e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TextVectorizer:\n",
    "    def __init__(self, max_tokens=VOCAB_SIZE, output_sequence_length=TEXT_SEQ_OUTPUT_LEN):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.output_sequence_length = output_sequence_length\n",
    "        self.vocab = {}\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "        self.pad_token_id = 0\n",
    "        self.unk_token_id = 1\n",
    "\n",
    "    def fit_on_texts(self, texts: List[str]):\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            word_counts.update(text.lower().split())\n",
    "\n",
    "        # Keep most common words, reserve for pad and unk\n",
    "        common_words = [word for word, count in word_counts.most_common(self.max_tokens - 2)]\n",
    "        self.token_to_idx = {PAD_TOKEN: self.pad_token_id, UNK_TOKEN: self.unk_token_id}\n",
    "        for i, token in enumerate(common_words):\n",
    "            self.token_to_idx[token] = i + 2  # Start after pad and unk\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "        self.vocab_size = len(self.token_to_idx)\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "\n",
    "    def texts_to_sequences(self, texts: List[str]) -> List[List[int]]:\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            tokens = text.lower().split()\n",
    "            seq = [self.token_to_idx.get(token, self.unk_token_id) for token in tokens]\n",
    "            # Pad or truncate individual row text sequence\n",
    "            if len(seq) < self.output_sequence_length:\n",
    "                seq.extend([self.pad_token_id] * (self.output_sequence_length - len(seq)))\n",
    "            else:\n",
    "                seq = seq[:self.output_sequence_length]\n",
    "            sequences.append(seq)\n",
    "        return sequences"
   ],
   "id": "6c192ba05d836af0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load annotations",
   "id": "2221e5ee8a083839"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "annotations_filename = \"annotationsv3.csv\"\n",
    "annotations_row_data = 'raw_cells_json'"
   ],
   "id": "b94a0a411363c4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TODO:\n",
    "- replace iterrows\n",
    "```\n",
    "feature_series = df_annotations['raw_cells_list'] \\\n",
    "    .apply(extract_single_row_features)\n",
    "df_processed = pd.DataFrame(feature_series.tolist())\n",
    "df_processed[['text','numerics']].apply(pd.Series)\n",
    "```"
   ],
   "id": "8391c322dccd31c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_annotations = pd.read_csv(annotations_filename)\n",
    "\n",
    "df_annotations['raw_cells_list'] = df_annotations[annotations_row_data].apply(\n",
    "    lambda x: json.loads(x) if pd.notna(x) else [])\n",
    "\n",
    "all_row_data = []\n",
    "for index, row in df_annotations.iterrows():\n",
    "    features = extract_single_row_features(row['raw_cells_list'])\n",
    "    all_row_data.append({\n",
    "        'text': features['concatenated_text_clean'],\n",
    "        'numerics': features['numeric_features'],\n",
    "        'label': row['label'],\n",
    "        'filename': row['filename'],\n",
    "        'sheetname': row['sheetname']\n",
    "    })\n",
    "df_processed_rows = pd.DataFrame(all_row_data)"
   ],
   "id": "d4ddcf2f328d111b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_processed_rows.head()",
   "id": "205fee15cc534e49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Group by sheet",
   "id": "26617ef40ffd52b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c0dfbc240b5da646"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
