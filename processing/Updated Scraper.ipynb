{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8c265e-337d-4ea8-a76e-2784f00b66aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# # Install the xlrd library\n",
    "%pip install xlrd\n",
    "%pip install xlwt\n",
    "%pip install pandas\n",
    "%pip install openpyxl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d65b227-7391-4445-92a5-f79dfb6b3943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import xlrd\n",
    "import xlwt\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f644364e-c68e-4016-9a9a-7acbbe03088f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "excel_files = [f for f in os.listdir(\"data/\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f3f713-9c96-452a-b483-a5304bd20334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "excel_files"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff7ae45-31a8-4ce1-b126-2af9c8db6f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "def extract_rdo_number(filename):\n",
    "    try:\n",
    "        # Use regular expressions to find the numeric part of the RDO number\n",
    "        match = re.search(r'RDO No\\. (\\d+)\\w? - (.+)\\.?(?:xls|xlsx)?', filename, re.IGNORECASE)\n",
    "        if match:\n",
    "            return int(match.group(1))  # Extract the number and convert to integer\n",
    "        else:\n",
    "            return float('inf')\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Error processing filename: {filename} - {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "# Sort the list using the extracted RDO number\n",
    "sorted_files = sorted(excel_files, key=extract_rdo_number)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a34ae3d-8f23-4dd7-a4f8-6ae7127dd38a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "sorted_files"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c7c758-b23f-4098-9efd-eba5676c3628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "excel_files.sort()\n",
    "excel_files"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b17edc-84e5-4f19-bf8f-9f76cc1d6b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "def xls_to_df(filename, base_dir=\"data/\"):\n",
    "    filepath = os.path.join(base_dir, filename)\n",
    "\n",
    "    # Check file extension and specify engine if necessary\n",
    "    if filename.lower().endswith('.xls'):\n",
    "        test = pd.ExcelFile(filepath, engine='xlrd')  # Use xlrd for .xls files\n",
    "    else:\n",
    "        test = pd.ExcelFile(filepath, engine='openpyxl')  # Use openpyxl for .xlsx files\n",
    "\n",
    "    sheet_names = test.sheet_names\n",
    "    last_sheet_name = None\n",
    "    \n",
    "    # Sort the sheet names if they follow the 'Sheet' naming pattern\n",
    "    sheet_names = sorted([name for name in sheet_names if name.strip().lower().startswith('sheet')],\n",
    "                         key=lambda name: int(re.search(r'\\d+', name).group()))\n",
    "    \n",
    "    # Select the last sheet that matches the pattern\n",
    "    if sheet_names:\n",
    "        last_sheet_name = sheet_names[-1]\n",
    "    \n",
    "    if last_sheet_name:\n",
    "        df = pd.read_excel(filepath, sheet_name=last_sheet_name, header=None)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"No matching sheets found in {filename}\")\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10de7818-f742-4464-be37-108dcd04eb30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "def clean_value(value, feature=False):\n",
    "    try:\n",
    "        float_value = float(value)\n",
    "        return round(float_value, 3)\n",
    "    except (ValueError, TypeError):\n",
    "        value = str(value)\n",
    "        if value == 'nan':\n",
    "            return ''\n",
    "        if value is not None:\n",
    "            value = re.sub(r\"^\\s*:\\s*\", \"\", value.strip())\n",
    "            if not feature:\n",
    "                value = re.sub(r\"(D\\.?\\s*O\\s*\\.?\\s*No|Effec(?:t)?ivity Date)\\s*.*\", \"\", value, flags=re.IGNORECASE).strip()\n",
    "            value = re.sub(r'^no\\.\\s*\\d+\\s*-\\s*', '', value, flags=re.IGNORECASE).strip()\n",
    "            value = re.sub(r\"\\s*-*\\s*(\\s*\\(cont\\s*\\.\\)|(?:\\()?\\s*continued\\s*(?:\\)?)|(?:\\()?\\s*continuation\\s*(?:\\))?|(?:\\()?\\s*continaution\\s*(?:\\))?)\", \"\", value, flags=re.IGNORECASE).strip()\n",
    "            value = re.sub(r'[\\s_]+$', '', value)\n",
    "            return value\n",
    "        return value"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_value(pattern, text):\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf089ff-e3bd-4c90-b76c-b4fa31f8dce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "def find_column_headers(df, index, proximity_window=6, debug=False):\n",
    "    import re\n",
    "    \n",
    "    headers = {\n",
    "        'street_name_index': None,\n",
    "        'vicinity_index': None,\n",
    "        'classification_index': None,\n",
    "        'zv_sq_m_index': None\n",
    "    }\n",
    "\n",
    "    headers_max_offset = {\n",
    "        'street_name_index': -1,\n",
    "        'vicinity_index': -1,\n",
    "        'classification_index': -1,\n",
    "        'zv_sq_m_index': -1\n",
    "    }\n",
    "\n",
    "    column_texts = {}\n",
    "    extend_search = False\n",
    "    offset = 0\n",
    "\n",
    "    zv_pattern_holder = None\n",
    "    zv_offset_holder = None\n",
    "    \n",
    "    classification_pattern_holder = None\n",
    "        \n",
    "    while offset < proximity_window:\n",
    "        current_index = index + offset\n",
    "        if current_index >= len(df):\n",
    "            break\n",
    "        \n",
    "        row = df.iloc[current_index]\n",
    "\n",
    "        for col_index, cell in enumerate(row):\n",
    "            cell_value = str(cell)\n",
    "            if col_index not in column_texts:\n",
    "                column_texts[col_index] = cell_value\n",
    "            else:\n",
    "                column_texts[col_index] += ' ' + cell_value\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Row {current_index}: {column_texts}\")\n",
    "\n",
    "        # Check each column's combined text for header patterns\n",
    "        for col_index, combined_text in column_texts.items():\n",
    "            if headers['street_name_index'] is None:\n",
    "                if re.search(\n",
    "                    r\"(S\\s*T\\s*R\\s*E\\s*E\\s*T\\s*N\\s*A\\s*M\\s*E|\"\n",
    "                    r\"S\\s*U\\s*B\\s*D\\s*I\\s*V\\s*I\\s*S\\s*I\\s*O\\s*N|\"\n",
    "                    r\"C\\s*O\\s*N\\s*D\\s*O\\s*M\\s*I\\s*N\\s*I\\s*U\\s*M)\",\n",
    "                    combined_text, re.IGNORECASE):\n",
    "                    headers['street_name_index'] = col_index\n",
    "                    headers_max_offset['street_name_index'] = offset\n",
    "                    if debug:\n",
    "                        print(f\"max offset updated: {current_index}\")\n",
    "\n",
    "            if headers['vicinity_index'] is None:\n",
    "                if re.search(r\"V\\s*I\\s*C\\s*I\\s*N\\s*I\\s*T\\s*Y\", combined_text, re.IGNORECASE):\n",
    "                    headers['vicinity_index'] = col_index\n",
    "                    headers_max_offset['vicinity_index'] = offset\n",
    "                    if debug:\n",
    "                        print(f\"max offset updated: {current_index}\")\n",
    "\n",
    "            if headers['classification_index'] is None:\n",
    "                if re.search(\n",
    "                    r\"CLASS(?:IFICATION)?|\"\n",
    "                    r\"C\\s*L\\s*A\\s*S\\s*S\\s*I\\s*F\\s*I\\s*C\\s*A\\s*T\\s*I\\s*O\\s*N\",\n",
    "                    combined_text, re.IGNORECASE | re.DOTALL):\n",
    "                    headers['classification_index'] = col_index\n",
    "                    headers_max_offset['classification_index'] = offset\n",
    "                    if debug:\n",
    "                        print(f\"max offset updated: {current_index}\")\n",
    "                    extend_search = True  # Flag to extend the search\n",
    "\n",
    "            if headers['zv_sq_m_index'] is None or headers['zv_sq_m_index'] < col_index:\n",
    "                zv_pattern = (\n",
    "                    r\"\\d+(?:ST|ND|RD|TH)\\s+(?:REVISION|Rev)(?:.*Z\\.?V\\.?.*SQ.*M\\.?)?|\"\n",
    "                    r\"(?:\\d+(?:ST|ND|RD|TH)\\s+REVISION|Rev\\s+ZV\\s+/?.*SQ\\.?\\s*M\\.?)|\"\n",
    "                    r\"(?:Z|2)\\.?V\\.?.*SQ.*M\\.?|FINAL\"\n",
    "                )\n",
    "                match = re.search(zv_pattern, combined_text, re.IGNORECASE) \n",
    "                if match:\n",
    "                    headers['zv_sq_m_index'] = col_index\n",
    "                    headers_max_offset['zv_sq_m_index'] = offset\n",
    "                    if debug:\n",
    "                        print(f\"max offset updated: {current_index}\")\n",
    "                    \n",
    "                    if not zv_pattern_holder: # if this is the first one\n",
    "                        zv_pattern_holder = match\n",
    "                        zv_offset_holder = offset\n",
    "                        headers['zv_sq_m_index'] = None\n",
    "                        extend_search = True  # extend the search\n",
    "                    elif zv_pattern_holder == match: # if new pattern is the same, get previous values\n",
    "                        headers_max_offset['zv_sq_m_index'] = zv_offset_holder\n",
    "                        \n",
    "                    # if the new match is different\n",
    "                    \n",
    "\n",
    "        if extend_search:\n",
    "            if debug:\n",
    "                print(\"Extending search\")\n",
    "            offset -= 2\n",
    "            index += 2\n",
    "            extend_search = False\n",
    "\n",
    "        offset += 1\n",
    "\n",
    "    # If all headers were found, determine the maximum offset used\n",
    "    if all(value is not None for value in headers.values()):\n",
    "        max_offset_used = max(headers_max_offset.values())\n",
    "        if debug:\n",
    "            print(f\"Headers found within proximity window up to row {index + max_offset_used}\")\n",
    "            print(f\"Header indices: {headers}\")\n",
    "        return True, headers, index + max_offset_used\n",
    "    else:\n",
    "        if debug:\n",
    "            print(f\"Headers not found within proximity window starting at index {index}\")\n",
    "        return False, None, index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2033bb79-e329-40ce-a9bc-7077c252f94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "def is_header_row(row):\n",
    "    header_patterns = [\n",
    "        r\"(S\\s*T\\s*R\\s*E\\s*E\\s*T\\s*N\\s*A\\s*M\\s*E|S\\s*U\\s*B\\s*D\\s*I\\s*V\\s*I\\s*S\\s*I\\s*O\\s*N|C\\s*O\\s*N\\s*D\\s*O\\s*M\\s*I\\s*N\\s*I\\s*U\\s*M)\",\n",
    "        r\"V.*I.*C.*I.*N.*I.*T.*Y\",\n",
    "        r\"CLASS(?:IFICATION)?|C.*L.*A.*S.*S.*I.*F.*I.*C.*A.*T.*I.*O.*N\",\n",
    "        r\"ZV.*SQ.*M|3rd\\s*Rev|FINAL\"\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    found = {pattern: False for pattern in header_patterns}\n",
    "    \n",
    "    for cell in row:\n",
    "        cell_value = str(cell)\n",
    "        for pattern in header_patterns:\n",
    "            if re.search(pattern, cell_value, re.IGNORECASE):\n",
    "                found[pattern] = True\n",
    "            \n",
    "    \n",
    "    return all(found.values())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "do = False\n",
    "off = 0\n",
    "a = 7\n",
    "while off < 5:\n",
    "    ca = a + off\n",
    "    print(f\"Ca, i: {ca}, {off}\")\n",
    "    if do:\n",
    "        off -= 2\n",
    "        ca += 2\n",
    "        do = False\n",
    "    off += 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09da24ed-d28e-4919-83ea-93e7cd1c5464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "def find_location_components(df, index, proximity_window=3, current_province=None, current_city=None, current_barangay=None, debug=False):\n",
    "    if debug:\n",
    "        print(f\"\\nRunning find_location_components\")\n",
    "    last_matched_index = index\n",
    "    initial_index = index\n",
    "    expecting_values = False  # Flag to indicate we are expecting values in subsequent rows after combined labels\n",
    "    found_any = False  # Flag to check if any location component is found\n",
    "    \n",
    "    extend_search = False\n",
    "    offset = 0\n",
    "    \n",
    "    \n",
    "    province_index = None\n",
    "    city_index = None\n",
    "    barangay_index = None\n",
    "    barangay_holder = None\n",
    "    city_holder = None\n",
    "    \n",
    "    while offset < proximity_window:\n",
    "        \n",
    "        current_index = index + offset\n",
    "        if current_index >= len(df):\n",
    "            break\n",
    "        current_row = df.iloc[current_index]\n",
    "        combined_current_row = ''.join(map(str, current_row.dropna())).strip()\n",
    "        non_null_cells = current_row.dropna().astype(str).tolist()\n",
    "        \n",
    "        if debug:\n",
    "            print(\"\\n\")\n",
    "            # print(f\"Row {current_index}: {non_null_cells}\")\n",
    "            # print(f\"Expeting values: {expecting_values}\")\n",
    "            print(f\"Searching row: {offset+1}/{proximity_window}\")\n",
    "\n",
    "        # Check if this row contains the combined labels\n",
    "        if not expecting_values and any(re.search(r\"PROVINCE\\s*/\\s*CITY\\s*/\\s*MUNICIPALITY\\s*/\\s*BARANGAYS\", cell, re.IGNORECASE) for cell in non_null_cells):\n",
    "            expecting_values = True\n",
    "            if debug:\n",
    "                print(f\"Combined labels found at row {current_index}\")\n",
    "            \n",
    "            # offset += 1\n",
    "            continue  # Move to the next row to read values\n",
    "\n",
    "        # If we're expecting values after combined labels\n",
    "        if expecting_values:\n",
    "            # Iterate over cells to find values starting with \":\"\n",
    "            for cell in non_null_cells:\n",
    "                cell = cell.strip()\n",
    "                if debug:\n",
    "                    print(f\"Cell: {non_null_cells}\")\n",
    "                    \n",
    "                if cell.startswith(\":\"):\n",
    "                    value = cell.lstrip(\":\").strip()\n",
    "                    if not current_province:\n",
    "                        current_province = clean_value(value)\n",
    "                        found_any = True\n",
    "                        if debug:\n",
    "                            print(f\"Province found: {current_province}\")\n",
    "                    elif not current_city:\n",
    "                        current_city = clean_value(value)\n",
    "                        found_any = True\n",
    "                        if debug:\n",
    "                            print(f\"City/Municipality found: {current_city}\")\n",
    "                    elif not current_barangay:\n",
    "                        current_barangay = clean_value(value)\n",
    "                        found_any = True\n",
    "                        if debug:\n",
    "                            print(f\"Barangay found: {current_barangay}\")\n",
    "            last_matched_index = current_index\n",
    "            # If all components have values (either found now or already had values), we can return\n",
    "            # if (current_province and current_city and current_barangay) or offset == proximity_window - 1:\n",
    "            if all([current_province and current_city and current_barangay]):\n",
    "                return current_province, current_city, current_barangay, last_matched_index\n",
    "            if offset == proximity_window - 1:\n",
    "                return current_province, current_city, current_barangay, initial_index\n",
    "            \n",
    "            offset += 1\n",
    "            continue  # Continue to next row to find remaining components\n",
    "        \n",
    "        \n",
    "        # Original logic for separate labels\n",
    "        else:\n",
    "            if combined_current_row.lower().startswith(\"district\"):\n",
    "                if debug:\n",
    "                    print(f\"Skipping row {current_index} as it starts with 'district'\") \n",
    "                offset += 1\n",
    "                continue\n",
    "            # Check for Province\n",
    "            province = extract_value(r\"Province\\s*(?::|\\s|of)?\\s*(.*)\", combined_current_row)\n",
    "            if province:\n",
    "                current_province = clean_value(province)\n",
    "                found_any = True\n",
    "                extend_search = True\n",
    "                last_matched_index = initial_index = province_index = current_index\n",
    "                if debug:\n",
    "                    print(f\"Province match found in row {current_index}: {current_province}\")\n",
    "\n",
    "            # Check for City/Municipality\n",
    "            city = extract_value(r\"(?:(?!City,)(?:City|Municipality))(?:\\s*\\/\\s*(?:City|Municipality))?\\s*[:\\s]?\\s*(.+)\", combined_current_row)\n",
    "            if city:\n",
    "                current_city = clean_value(city)\n",
    "                found_any = True\n",
    "                extend_search = True\n",
    "                last_matched_index = initial_index = city_index = current_index\n",
    "                if debug:\n",
    "                    print(f\"City/Municipality match found in row {current_index}: {current_city}\")\n",
    "\n",
    "            # Check for Barangay/Zone\n",
    "            barangay = extract_value(r\"(?:Barangays|Zone|Barangay)(?:\\s*\\/\\s*(?:Barangays|Zone|Barangay))?\\s*[:\\s]?\\s*(.+)\", combined_current_row)\n",
    "            # Check if the extracted barangay value contains a phrase like \"along barangay road\"\n",
    "            if barangay and re.search(r\".*\\s*(?:along\\s*)?barangay.*road.*\", combined_current_row, re.IGNORECASE):\n",
    "                # print(f\"Discarding match due to 'along barangay road' pattern: {barangay}\")\n",
    "                barangay = None\n",
    "            if barangay:\n",
    "                current_barangay = clean_value(barangay)\n",
    "                found_any = True\n",
    "                extend_search = True\n",
    "                last_matched_index = initial_index = barangay_index = current_index\n",
    "                if debug:\n",
    "                    print(f\"Barangay/Zone match found in row {current_index}: {current_barangay}\")\n",
    "           \n",
    "            if extend_search:\n",
    "                # print(\"Extending search\")\n",
    "                offset -= 2\n",
    "                index += 2\n",
    "                extend_search = False     \n",
    "\n",
    "            # If we've found any component, we can check if we've reached the proximity window or if all components are found\n",
    "            if found_any and all([current_province and current_city and current_barangay]):\n",
    "                # if barangay index is before province index, look for a province pa, and if we find, overwrite\n",
    "                if barangay_index is not None and province_index is not None and barangay_index < province_index and not barangay_holder:\n",
    "                    if debug:\n",
    "                        print(\"Extending search for new baranagay\")\n",
    "                    barangay_holder = current_barangay\n",
    "                    current_barangay = None\n",
    "                    offset -= 1\n",
    "                    index += 2\n",
    "                    continue\n",
    "                # Similarly, if city index is before province index, look for a province and overwrite\n",
    "                if city_index is not None and province_index is not None and city_index < province_index and not city_holder:\n",
    "                    if debug:\n",
    "                        print(\"Extending search for new city\")\n",
    "                    city_holder = current_city\n",
    "                    current_city = None\n",
    "                    offset -= 1\n",
    "                    index += 2\n",
    "                    continue\n",
    "                if debug:\n",
    "                    print(f\"Found all location components! Last matched index: {last_matched_index}\")\n",
    "                return current_province, current_city, current_barangay, last_matched_index            \n",
    "            \n",
    "            if offset == proximity_window - 1:\n",
    "                if found_any:\n",
    "                   return current_province, current_city, current_barangay, last_matched_index   \n",
    "                if barangay_holder:\n",
    "                    current_barangay = barangay_holder\n",
    "                    return current_province, current_city, current_barangay, last_matched_index\n",
    "                if city_holder:\n",
    "                    current_city = city_holder\n",
    "                    return current_province, current_city, current_barangay, last_matched_index\n",
    "                return current_province, current_city, current_barangay, initial_index\n",
    "            \n",
    "            # if extend_search:\n",
    "            #     # print(\"Extending search\")\n",
    "            #     offset -= 2\n",
    "            #     index += 2\n",
    "            #     extend_search = False\n",
    "        \n",
    "        offset += 1\n",
    "        if not expecting_values and not found_any:\n",
    "            break\n",
    "\n",
    "    return current_province, current_city, current_barangay, last_matched_index\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4feb705-9914-4ed1-a53b-9576f2c9285b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Code for Standard Format\n",
    "\n",
    "Standard Format follows: \n",
    "\n",
    "- each tables have complete Province, Barangay, City/mun and are in succeeding rows.\n",
    "                \n",
    "- Table headers can be can be read with this regex pattern: \n",
    "\n",
    "          r\"(STREET NAME|SUBDIVISION|CONDOMINIUM)\",\n",
    "          r\"V.*I.*C.*I.*N.*I.*T.*Y\",\n",
    "          r\"CLASS(?:IFICATION)?|C.*L.*A.*S.*S.*I.*F.*I.*C.*A.*T.*I.*O.*N\",\n",
    "          r\"ZV.*SQ.*M|3rd\\s*Rev\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8bfc63b-1bc3-4074-97c5-7c20240a6f75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "def main(df, debug=False, start=0, end=-1, debug_location=False, debug_header=False):\n",
    "    if end == -1:\n",
    "        final_index = len(df)\n",
    "    else:\n",
    "        final_index = end\n",
    "    index = start\n",
    "    count = 0\n",
    "    new_df = pd.DataFrame(columns=['Province', 'City/Municipality', 'Barangay', \n",
    "                                   'Street/Subdivision', 'Vicinity', 'Classification', 'ZV/SQM'])\n",
    "    \n",
    "    PROXIMITY_WINDOW = 2  # Increased to accommodate different formats\n",
    "    \n",
    "    # \n",
    "    current_province = None\n",
    "    current_city = None\n",
    "    current_barangay = None\n",
    "    header_indices = None\n",
    "    \n",
    "    continuation = False \n",
    "    # prev is previous table, holder is local table\n",
    "    prev_col1 = None\n",
    "    prev_vicinity = None\n",
    "    prev_classification = None\n",
    "    prev_zvsqm = None\n",
    "\n",
    "    # while index < len(df):\n",
    "    while index < final_index:\n",
    "        current_province_new, current_city_new, current_barangay_new, index  = find_location_components(\n",
    "            df, index, proximity_window=PROXIMITY_WINDOW, debug=debug_location)\n",
    "        # Update current location components with any new values\n",
    "        \n",
    "        found_components = any([current_province_new, current_city_new, current_barangay_new])\n",
    "        if found_components and debug:\n",
    "            print(f\"Location components found: {current_province_new}, {current_city_new}, {current_barangay_new}\")\n",
    "\n",
    "        # Attempt to find headers starting from the last matched index\n",
    "        found_headers, header_indices_new, new_index = find_column_headers(df, index, debug=debug_header)\n",
    "        if debug:\n",
    "            print(f\"Column headers found: {header_indices_new}\")\n",
    "        \n",
    "        # if we (kinda) confident we have a table\n",
    "        if found_headers and found_components:\n",
    "            if current_province_new == current_province:\n",
    "                continuation = True\n",
    "            else:\n",
    "                continuation = False\n",
    "            current_province = current_province_new if current_province_new else current_province\n",
    "            current_city = current_city_new if current_city_new else current_city\n",
    "            current_barangay = current_barangay_new if current_barangay_new else current_barangay\n",
    "            \n",
    "            # Update header indices\n",
    "            header_indices = header_indices_new\n",
    "            index = new_index  # Move index to after headers\n",
    "\n",
    "            # Start processing data rows\n",
    "            count += 1\n",
    "            if debug:\n",
    "                print(f'Processing table {count}\\n')\n",
    "\n",
    "            age = 0\n",
    "            MAX_AGE = 4\n",
    "            col1_holder = None\n",
    "            vicinity_holder = None\n",
    "\n",
    "            all_other_vicinity = None\n",
    "\n",
    "            while index < final_index and age < MAX_AGE:\n",
    "                # TODO: Check the types of all variables because some NaN stuff and floats and inconsistent and yeah\n",
    "                row = df.iloc[index]\n",
    "                \n",
    "                vicinity = 'Test u should not see this pop up pls'\n",
    "                # Extract data using the header indices\n",
    "                col1 = row.iloc[header_indices['street_name_index']]\n",
    "                classification = row.iloc[header_indices['classification_index']]\n",
    "                zv = row.iloc[header_indices['zv_sq_m_index']]\n",
    "                \n",
    "                # Check for double column\n",
    "                if isinstance(header_indices['vicinity_index'], int):\n",
    "                    vicinity = row.iloc[header_indices['vicinity_index']]\n",
    "                elif isinstance(header_indices['vicinity_index'], list):\n",
    "                    vicinity1 = str(row.iloc[header_indices['vicinity_index'][0]])\n",
    "                    vicinity2 = str(row.iloc[header_indices['vicinity_index'][1]])\n",
    "                    if vicinity1 == 'nan':\n",
    "                        vicinity = vicinity2\n",
    "                    elif vicinity2 == 'nan':\n",
    "                        vicinity = vicinity1\n",
    "                    else:\n",
    "                        vicinity = f\"{vicinity1}, {vicinity2}\"\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"Data row at index {index}: {[col1, vicinity, classification, zv]}\")\n",
    "                    # print(f'vicinity header index: {header_indices[\"vicinity_index\"]}')\n",
    "                    \n",
    "                \n",
    "\n",
    "                # Check for new location components in the current row\n",
    "                current_province_new_in_row, current_city_new_in_row, current_barangay_new_in_row, new_index_2 = find_location_components(\n",
    "                    df, index, proximity_window=PROXIMITY_WINDOW, debug=debug_location)\n",
    "                # found_headers_in_row, header_indices_in_row, new_index_in_row = find_column_headers(df, index, debug=debug)\n",
    "                \n",
    "                # if col1 index is not zone/barangay pattern\n",
    "                # if barangay index is before province index, look for a province pa, and if we find, overwrite\n",
    "                found_headers_in_row, header_indices_in_row, new_index_in_row = find_column_headers(df, new_index_2, debug=debug_header)\n",
    "                \n",
    "                combined_row = ''.join(map(str, row[[header_indices['classification_index'], header_indices['zv_sq_m_index']]].dropna())).strip()\n",
    "                valid_data_row = clean_value(combined_row)\n",
    "                if debug and any([current_province_new_in_row, current_city_new_in_row, current_barangay_new_in_row]):\n",
    "                    if current_province_new_in_row:\n",
    "                        print(f\"Province found: {current_province_new_in_row}\")\n",
    "                    if current_city_new_in_row:\n",
    "                        print(f\"City/Municipality found: {current_city_new_in_row}\")\n",
    "                    if current_barangay_new_in_row:\n",
    "                        print(f\"Barangay found: {current_barangay_new_in_row}\")\n",
    "                    if found_headers_in_row:\n",
    "                        print(\"Column headers found\")\n",
    "                    print(f\"Valid data row: {valid_data_row}\")\n",
    "                        \n",
    "                \n",
    "                # TODO: revisit this condition for new table\n",
    "                # print(f\"Validity: {valid_data_row}\")\n",
    "                if not valid_data_row and (any([current_province_new_in_row, current_city_new_in_row, current_barangay_new_in_row]) and found_headers_in_row):\n",
    "                    # End current table processing\n",
    "                    if debug:\n",
    "                        print(f\"New location and headers found at index {index}. Ending current table and starting new table.\")\n",
    "                        print(f\"current_province: {current_province_new_in_row}, current_city: {current_city_new_in_row}, current_barangay: {current_barangay_new_in_row}\")\n",
    "                    # Update location components\n",
    "                    if current_province_new_in_row == current_province:\n",
    "                        continuation = True\n",
    "                    else:\n",
    "                        continuation = False\n",
    "                    current_province = current_province_new_in_row if current_province_new_in_row else current_province\n",
    "                    current_city = current_city_new_in_row if current_city_new_in_row else current_city\n",
    "                    current_barangay = current_barangay_new_in_row if current_barangay_new_in_row else current_barangay\n",
    "                    \n",
    "                    # Update headers\n",
    "                    header_indices = header_indices_in_row\n",
    "                    index = new_index_in_row  # Move index to after headers\n",
    "                    \n",
    "                    # Reset variables\n",
    "                    age = 0\n",
    "                    col1_holder = None\n",
    "                    vicinity_holder = None\n",
    "                    count += 1  # Increment table count\n",
    "                    if debug:\n",
    "                        print('\\n' + '#' * 60)\n",
    "                        print('#' + ' ' * 58 + '#')\n",
    "                        print('#{:^58}#'.format(f'>>> PROCESSING TABLE {count} <<<'))\n",
    "                        print('#' + ' ' * 58 + '#')\n",
    "                        print('#' * 60 + '\\n')\n",
    "\n",
    "                    continue  # Start processing new table from updated index\n",
    "                \n",
    "                cleaned_row = clean_value(''.join(map(str, row.dropna())).strip())\n",
    "                row_is_valid = (not ((pd.isnull(classification) or str(classification).strip() == '') and (pd.isnull(zv) or str(zv).strip() == ''))) and str(cleaned_row).strip() \n",
    "                if not row_is_valid:\n",
    "                    index += 1\n",
    "                    age += 1\n",
    "                    continue \n",
    "                    \n",
    "                # Check if both classification and ZV/SQM are empty\n",
    "                if (pd.isnull(classification) or str(classification).strip() == '') and (pd.isnull(zv) or str(zv).strip() == ''):\n",
    "                    index += 1\n",
    "                    age += 1\n",
    "                    continue  \n",
    "                \n",
    "                if str(classification).strip().lower() == 'nan' and not str(\"ZV / SQ. M\").replace('.', '', 1).isdigit():\n",
    "                    index += 1\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                # Checking for empty col1\n",
    "                null_col1 = pd.isna(col1) or not str(col1).strip()\n",
    "                if null_col1:  \n",
    "                    if continuation:\n",
    "                        col1 = col1_holder if not (pd.isna(col1_holder) or not str(col1_holder).strip()) else prev_col1 \n",
    "                    elif not (pd.isna(col1_holder) or not str(col1_holder).strip()):\n",
    "                        col1 = col1_holder\n",
    "                else:\n",
    "                    col1_holder = col1\n",
    "                \n",
    "                if isinstance(col1, str):\n",
    "                    col1_stripped_upper = col1.strip().upper()\n",
    "                    is_all_other = col1_stripped_upper.startswith(\"ALL OTHER\")\n",
    "                else:\n",
    "                    col1_stripped_upper = ''\n",
    "                    is_all_other = False\n",
    "                               \n",
    "                # Check if 'vicinity' is null or empty\n",
    "                null_vicinity = pd.isna(vicinity) or not str(vicinity).strip()\n",
    "                if null_vicinity: # if vicinity is null\n",
    "                    if continuation: # if the table is a continuation\n",
    "                        if not (pd.isna(prev_col1) and pd.isna(col1)) and prev_col1 != col1: # if new col1\n",
    "                            vicinity_holder = vicinity #update the holder\n",
    "                        else:\n",
    "                            vicinity = vicinity_holder if not (pd.isna(vicinity_holder) or not str(vicinity_holder).strip()) else prev_vicinity\n",
    "                    elif not (pd.isna(vicinity_holder) or not str(vicinity_holder).strip()):\n",
    "                        if not (pd.isna(prev_col1) and pd.isna(col1)) and prev_col1 != col1: # if new col1\n",
    "                            vicinity_holder = vicinity\n",
    "                        else:\n",
    "                            vicinity = vicinity_holder \n",
    "                else:\n",
    "                    vicinity_holder = vicinity  \n",
    "                    \n",
    "                # 'ALL OTHER' logic\n",
    "                if is_all_other:\n",
    "                    if not null_vicinity:\n",
    "                        all_other_vicinity = vicinity\n",
    "                    if all_other_vicinity:\n",
    "                        vicinity = all_other_vicinity\n",
    "                    else:\n",
    "                        vicinity = ''\n",
    "                        if debug:\n",
    "                            print(f\"'col1' starts with 'ALL OTHER'. Setting 'vicinity' to blank.\")\n",
    "                else:\n",
    "                    all_other_vicinity = None\n",
    "                \n",
    "                def is_dash_string(var):\n",
    "                    return isinstance(var, str) and re.fullmatch(r\"\\-+\", var) is not None\n",
    "                \n",
    "                matches = sum(is_dash_string(var) for var in [col1, vicinity, classification, zv])\n",
    "                if matches >= 3:\n",
    "                    index += 1\n",
    "                    age += 1\n",
    "                    continue\n",
    "\n",
    "                # Append to new DataFrame\n",
    "                # TODO: check if cleaning features is necessary\n",
    "                new_df.loc[len(new_df)] = [\n",
    "                    current_province, \n",
    "                    current_city, \n",
    "                    current_barangay, \n",
    "                    clean_value(col1, feature=True), \n",
    "                    clean_value(vicinity, feature=True), \n",
    "                    clean_value(classification, feature=True), \n",
    "                    clean_value(zv, feature=True)\n",
    "                ]\n",
    "                \n",
    "                prev_col1 = col1\n",
    "                prev_vicinity = vicinity\n",
    "                prev_classification = classification\n",
    "                prev_zvsqm = zv\n",
    "                \n",
    "                if debug:\n",
    "                    print(new_df.loc[len(new_df)-1])\n",
    "                    print(\"\\n-------\\n\")\n",
    "                \n",
    "                index += 1\n",
    "                age = 0\n",
    "            continue  # Proceed to next iteration of the main loop\n",
    "        else:\n",
    "            index += 1  # No headers found, move to the next row\n",
    "    if debug:\n",
    "        print(f\"Total tables processed: {count}\")\n",
    "    return new_df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdf3400-7d38-45f1-86b1-184d7f0da523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "str('nan').strip().lower() == 'nan' and not str(\"ZV / SQ. M\").replace('.', '', 1).isdigit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "excel_files = [f for f in os.listdir(\"data/\") if os.path.isfile(os.path.join(\"data/\", f))]\n",
    "excel_files[110:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63954095-e2da-479f-8f0c-453902ada7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Running for non-QC\n",
    "os.makedirs(\"Output\", exist_ok=True)\n",
    "excel_files = [f for f in os.listdir(\"data/\") if os.path.isfile(os.path.join(\"data/\", f))]\n",
    "\n",
    "for excel in excel_files:\n",
    "    print(f'Processing {excel}')\n",
    "    df = xls_to_df(excel)\n",
    "    processed = main(df)\n",
    "    \n",
    "    # Split the filename and the extension\n",
    "    filename, extension = os.path.splitext(excel)\n",
    "    \n",
    "    if extension.lower() == '.xls':\n",
    "        normalized_filename = f\"{filename}.xlsx\"  \n",
    "    elif extension.lower() == '.xlsx':\n",
    "        normalized_filename = excel  \n",
    "    else:\n",
    "        print(f\"Unsupported file format for {excel}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    output_path = os.path.join(\"Output\", f\"Updated_{normalized_filename}\")\n",
    "    processed.to_excel(output_path, index=False)\n",
    "\n",
    "    print(f'Processed file saved as: {output_path}')\n",
    "\n",
    "print(\"done w all nice\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3705b6b2-5f25-4266-a945-93c30c1cc1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "output_files = [f for f in os.listdir(\"Output\")]\n",
    "for output_file in output_files:\n",
    "    file = pd.read_excel(f'Output/{output_file}')\n",
    "    if len(file) < 2:\n",
    "        print(output_file[:-5])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d62086-8689-4fce-b104-9c1f368c8b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note These files above have the vicinity in weird unmerged basta"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526cb26c-b982-4f8d-861b-56aa8da67765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "file = 'RDO No. 43 - Pasig City.xls'\n",
    "test = pd.ExcelFile(f\"data/{file}\")\n",
    "\n",
    "sheet_names = sorted([name for name in test.sheet_names if name.strip().lower().startswith('sheet')], key=lambda name: int(re.search(r'\\d+', name).group()))\n",
    "last_sheet_name = sheet_names[-1] if sheet_names else None\n",
    "\n",
    "df = pd.read_excel(\"data/\"+file, sheet_name=last_sheet_name, header=None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main(df, debug=True, start=1411, end=1555, debug_location=False, debug_header=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main(df, debug=True, start=0, end=555, debug_location=False, debug_header=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main(df, debug=True, start=0, end=1555, debug_location=False, debug_header=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.iloc[927]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "find_location_components(df, 927, debug=True, proximity_window=2)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "find_column_headers(df, 1414, debug=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59949e4-bade-4705-8503-1a4164e0f279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "rdo_list = [\n",
    "    \"Updated_RDO NO. 113A - West Davao City\",\n",
    "    \"Updated_RDO No. 24 - Valenzuela City\",\n",
    "    \"Updated_RDO No. 26 - Malabon-Navotas\",\n",
    "    \"Updated_RDO No. 29 - Tondo-San Nicolas\",\n",
    "    \"Updated_RDO No. 30 - Binondo\",\n",
    "    \"Updated_RDO No. 48 - West Makati\",\n",
    "    \"Updated_RDO No. 49 - North Makati City\",\n",
    "    \"Updated_RDO No. 74 - Iloilo City, Iloilo\",\n",
    "    \"Updated_RDO No. 80 - Mandaue City, Cebu\",\n",
    "    \"Updated_RDO No. 87 - Catbalogan City\",\n",
    "    \"Updated_RDO No. 98 - Cagayan de Oro City, Misamis Oriental\"\n",
    "]\n",
    "\n",
    "sorted(rdo_list, key=extract_rdo_number)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.iloc[1416]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main_recursive(df, debug=False, start=0, end=-1, debug_location=False, debug_header=False):\n",
    "    \"\"\"\n",
    "    A recursive version of the main function for processing tables in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame to process\n",
    "        debug (bool): Enable debug output\n",
    "        start (int): Starting index in DataFrame\n",
    "        end (int): Ending index in DataFrame (-1 for end of DataFrame)\n",
    "        debug_location (bool): Enable debug output for location components\n",
    "        debug_header (bool): Enable debug output for header finding\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Processed data with structured columns\n",
    "    \"\"\"\n",
    "    if end == -1:\n",
    "        final_index = len(df)\n",
    "    else:\n",
    "        final_index = end\n",
    "    \n",
    "    # Initialize the new DataFrame\n",
    "    new_df = pd.DataFrame(columns=['Province', 'City/Municipality', 'Barangay', \n",
    "                                   'Street/Subdivision', 'Vicinity', 'Classification', 'ZV/SQM'])\n",
    "    \n",
    "    # Start recursion\n",
    "    result_df, count = process_tables_recursive(\n",
    "        df, \n",
    "        new_df, \n",
    "        start, \n",
    "        final_index, \n",
    "        None, None, None,  # Current province, city, barangay\n",
    "        None, None, None, None,  # Previous values\n",
    "        0,  # Table count\n",
    "        False,  # Continuation flag\n",
    "        debug, debug_location, debug_header\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Total tables processed: {count}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def process_tables_recursive(df, result_df, index, final_index, \n",
    "                           current_province, current_city, current_barangay,\n",
    "                           prev_col1, prev_vicinity, prev_classification, prev_zvsqm,\n",
    "                           count, continuation, debug, debug_location, debug_header):\n",
    "    \"\"\"\n",
    "    Recursive function to process tables in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame\n",
    "        result_df (pandas.DataFrame): Output DataFrame being built\n",
    "        index (int): Current position in the DataFrame\n",
    "        final_index (int): End position to process\n",
    "        current_province, current_city, current_barangay: Current location components\n",
    "        prev_col1, prev_vicinity, prev_classification, prev_zvsqm: Previous values\n",
    "        count (int): Count of tables processed so far\n",
    "        continuation (bool): Whether this table is a continuation of the previous\n",
    "        debug, debug_location, debug_header (bool): Debug flags\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (result_df, count) - The updated DataFrame and table count\n",
    "    \"\"\"\n",
    "    # Base case: end of DataFrame or reached final index\n",
    "    if index >= final_index:\n",
    "        return result_df, count\n",
    "    \n",
    "    PROXIMITY_WINDOW = 2  # Increased to accommodate different formats\n",
    "    \n",
    "    # Find location components\n",
    "    current_province_new, current_city_new, current_barangay_new, index = find_location_components(\n",
    "        df, index, proximity_window=PROXIMITY_WINDOW, debug=debug_location)\n",
    "    \n",
    "    found_components = any([current_province_new, current_city_new, current_barangay_new])\n",
    "    if found_components and debug:\n",
    "        print(f\"Location components found: {current_province_new}, {current_city_new}, {current_barangay_new}\")\n",
    "    \n",
    "    # Attempt to find headers starting from the last matched index\n",
    "    found_headers, header_indices, new_index = find_column_headers(df, index, debug=debug_header)\n",
    "    if debug:\n",
    "        print(f\"Column headers found: {header_indices}\")\n",
    "    \n",
    "    # If we found both headers and location components\n",
    "    if found_headers and found_components:\n",
    "        # Update continuation flag\n",
    "        if current_province_new == current_province:\n",
    "            continuation = True\n",
    "        else:\n",
    "            continuation = False\n",
    "        \n",
    "        # Update current location\n",
    "        current_province = current_province_new if current_province_new else current_province\n",
    "        current_city = current_city_new if current_city_new else current_city\n",
    "        current_barangay = current_barangay_new if current_barangay_new else current_barangay\n",
    "        \n",
    "        # Move index to after headers\n",
    "        index = new_index\n",
    "        \n",
    "        # Increment table count\n",
    "        count += 1\n",
    "        if debug:\n",
    "            print(f'Processing table {count}\\n')\n",
    "        \n",
    "        # Process the data rows\n",
    "        result = process_data_rows_recursive(\n",
    "            df, result_df, index, final_index, \n",
    "            current_province, current_city, current_barangay, \n",
    "            header_indices, \n",
    "            prev_col1, prev_vicinity, prev_classification, prev_zvsqm, \n",
    "            continuation, \n",
    "            debug, debug_location, debug_header\n",
    "        )\n",
    "        \n",
    "        # Unpack the result\n",
    "        result_df = result[0]\n",
    "        index = result[1]\n",
    "        new_prev_col1 = result[2]\n",
    "        new_prev_vicinity = result[3]\n",
    "        new_prev_classification = result[4]\n",
    "        new_prev_zvsqm = result[5]\n",
    "        found_new_table = result[6]\n",
    "        \n",
    "        # If a new table was found, handle it\n",
    "        if found_new_table:\n",
    "            # Get the new table information\n",
    "            new_province = result[7]\n",
    "            new_city = result[8]\n",
    "            new_barangay = result[9]\n",
    "            new_headers = result[10]\n",
    "            \n",
    "            # Update for the new table found within data rows\n",
    "            if debug:\n",
    "                print('\\n' + '#' * 60)\n",
    "                print('#' + ' ' * 58 + '#')\n",
    "                print('#{:^58}#'.format(f'>>> PROCESSING TABLE {count+1} <<<'))\n",
    "                print('#' + ' ' * 58 + '#')\n",
    "                print('#' * 60 + '\\n')\n",
    "            \n",
    "            # Recursively process the new table\n",
    "            return process_tables_recursive(\n",
    "                df, result_df, index, final_index, \n",
    "                new_province, new_city, new_barangay, \n",
    "                new_prev_col1, new_prev_vicinity, new_prev_classification, new_prev_zvsqm, \n",
    "                count + 1, # Increment table count\n",
    "                True if new_province == current_province else False, # Set continuation flag\n",
    "                debug, debug_location, debug_header\n",
    "            )\n",
    "        \n",
    "        # Continue to next table (no new table was found within the data rows)\n",
    "        return process_tables_recursive(\n",
    "            df, result_df, index, final_index, \n",
    "            current_province, current_city, current_barangay, \n",
    "            new_prev_col1, new_prev_vicinity, new_prev_classification, new_prev_zvsqm, \n",
    "            count, continuation, \n",
    "            debug, debug_location, debug_header\n",
    "        )\n",
    "    else:\n",
    "        # No headers or location found, move to next row\n",
    "        return process_tables_recursive(\n",
    "            df, result_df, index + 1, final_index, \n",
    "            current_province, current_city, current_barangay, \n",
    "            prev_col1, prev_vicinity, prev_classification, prev_zvsqm, \n",
    "            count, continuation, \n",
    "            debug, debug_location, debug_header\n",
    "        )\n",
    "\n",
    "def process_data_rows_recursive(df, result_df, index, final_index, \n",
    "                              current_province, current_city, current_barangay, \n",
    "                              header_indices, \n",
    "                              prev_col1, prev_vicinity, prev_classification, prev_zvsqm, \n",
    "                              continuation, \n",
    "                              debug, debug_location, debug_header,\n",
    "                              age=0, col1_holder=None, vicinity_holder=None, all_other_vicinity=None):\n",
    "    \"\"\"\n",
    "    Recursively process data rows within a found table.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (result_df, next_index, prev_col1, prev_vicinity, prev_classification, prev_zvsqm, \n",
    "                found_new_table, new_province, new_city, new_barangay, new_headers)\n",
    "    \"\"\"\n",
    "    # Base cases\n",
    "    MAX_AGE = 4\n",
    "    if index >= final_index:\n",
    "        return (result_df, index, prev_col1, prev_vicinity, prev_classification, prev_zvsqm, \n",
    "                False, None, None, None, None)\n",
    "    \n",
    "    if age >= MAX_AGE:\n",
    "        return (result_df, index, prev_col1, prev_vicinity, prev_classification, prev_zvsqm, \n",
    "                False, None, None, None, None)\n",
    "    \n",
    "    # Get current row\n",
    "    row = df.iloc[index]\n",
    "    \n",
    "    # Extract data using the header indices\n",
    "    col1 = row.iloc[header_indices['street_name_index']]\n",
    "    classification = row.iloc[header_indices['classification_index']]\n",
    "    zv = row.iloc[header_indices['zv_sq_m_index']]\n",
    "    \n",
    "    # Handle vicinity (which could be a single column or two columns)\n",
    "    vicinity = 'Test u should not see this pop up pls'\n",
    "    if isinstance(header_indices['vicinity_index'], int):\n",
    "        vicinity = row.iloc[header_indices['vicinity_index']]\n",
    "    elif isinstance(header_indices['vicinity_index'], list):\n",
    "        vicinity1 = str(row.iloc[header_indices['vicinity_index'][0]])\n",
    "        vicinity2 = str(row.iloc[header_indices['vicinity_index'][1]])\n",
    "        if vicinity1 == 'nan':\n",
    "            vicinity = vicinity2\n",
    "        elif vicinity2 == 'nan':\n",
    "            vicinity = vicinity1\n",
    "        else:\n",
    "            vicinity = f\"{vicinity1}, {vicinity2}\"\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Data row at index {index}: {[col1, vicinity, classification, zv]}\")\n",
    "    \n",
    "    # Check for new location components in the current row\n",
    "    current_province_new_in_row, current_city_new_in_row, current_barangay_new_in_row, new_index_2 = find_location_components(\n",
    "        df, index, debug=debug_location)\n",
    "    \n",
    "    # Check for new headers\n",
    "    found_headers_in_row, header_indices_in_row, new_index_in_row = find_column_headers(df, new_index_2, debug=debug_header)\n",
    "    \n",
    "    # Check if row has valid data\n",
    "    combined_row = ''.join(map(str, row[[header_indices['classification_index'], header_indices['zv_sq_m_index']]].dropna())).strip()\n",
    "    valid_data_row = clean_value(combined_row)\n",
    "    \n",
    "    if debug and any([current_province_new_in_row, current_city_new_in_row, current_barangay_new_in_row]):\n",
    "        if current_province_new_in_row:\n",
    "            print(f\"Province found: {current_province_new_in_row}\")\n",
    "        if current_city_new_in_row:\n",
    "            print(f\"City/Municipality found: {current_city_new_in_row}\")\n",
    "        if current_barangay_new_in_row:\n",
    "            print(f\"Barangay found: {current_barangay_new_in_row}\")\n",
    "        if found_headers_in_row:\n",
    "            print(\"Column headers found\")\n",
    "        print(f\"Valid data row: {valid_data_row}\")\n",
    "    \n",
    "    # Check if we found a new table\n",
    "    if not valid_data_row and (any([current_province_new_in_row, current_city_new_in_row, current_barangay_new_in_row]) and found_headers_in_row):\n",
    "        if debug:\n",
    "            print(f\"New location and headers found at index {index}. Ending current table and starting new table.\")\n",
    "            print(f\"current_province: {current_province_new_in_row}, current_city: {current_city_new_in_row}, current_barangay: {current_barangay_new_in_row}\")\n",
    "        \n",
    "        # Update location components\n",
    "        next_province = current_province_new_in_row if current_province_new_in_row else current_province\n",
    "        next_city = current_city_new_in_row if current_city_new_in_row else current_city\n",
    "        next_barangay = current_barangay_new_in_row if current_barangay_new_in_row else current_barangay\n",
    "        \n",
    "        # Return signal to start a new table\n",
    "        return (result_df, new_index_in_row, prev_col1, prev_vicinity, prev_classification, prev_zvsqm, \n",
    "                True, next_province, next_city, next_barangay, header_indices_in_row)\n",
    "    \n",
    "    # Check if the row contains valid data\n",
    "    cleaned_row = clean_value(''.join(map(str, row.dropna())).strip())\n",
    "    row_is_valid = (not ((pd.isnull(classification) or str(classification).strip() == '') and \n",
    "                        (pd.isnull(zv) or str(zv).strip() == ''))) and str(cleaned_row).strip()\n",
    "    \n",
    "    if not row_is_valid:\n",
    "        # Skip this row and increase age\n",
    "        return process_data_rows_recursive(\n",
    "            df, result_df, index + 1, final_index, \n",
    "            current_province, current_city, current_barangay, \n",
    "            header_indices, \n",
    "            prev_col1, prev_vicinity, prev_classification, prev_zvsqm, \n",
    "            continuation, \n",
    "            debug, debug_location, debug_header, \n",
    "            age + 1, col1_holder, vicinity_holder, all_other_vicinity\n",
    "        )\n",
    "    \n",
    "    # Check if both classification and ZV/SQM are empty\n",
    "    if (pd.isnull(classification) or str(classification).strip() == '') and (pd.isnull(zv) or str(zv).strip() == ''):\n",
    "        # Skip this row and increase age\n",
    "        return process_data_rows_recursive(\n",
    "            df, result_df, index + 1, final_index, \n",
    "            current_province, current_city, current_barangay, \n",
    "            header_indices, \n",
    "            prev_col1, prev_vicinity, prev_classification, prev_zvsqm, \n",
    "            continuation, \n",
    "            debug, debug_location, debug_header, \n",
    "            age + 1, col1_holder, vicinity_holder, all_other_vicinity\n",
    "        )\n",
    "    \n",
    "    if str(classification).strip().lower() == 'nan' and not str(\"ZV / SQ. M\").replace('.', '', 1).isdigit():\n",
    "        # Skip this row\n",
    "        return process_data_rows_recursive(\n",
    "            df, result_df, index + 1, final_index, \n",
    "            current_province, current_city, current_barangay, \n",
    "            header_indices, \n",
    "            prev_col1, prev_vicinity, prev_classification, prev_zvsqm, \n",
    "            continuation, \n",
    "            debug, debug_location, debug_header, \n",
    "            age, col1_holder, vicinity_holder, all_other_vicinity\n",
    "        )\n",
    "    \n",
    "    # Handle col1\n",
    "    null_col1 = pd.isna(col1) or not str(col1).strip()\n",
    "    if null_col1:\n",
    "        if continuation:\n",
    "            col1 = col1_holder if not (pd.isna(col1_holder) or not str(col1_holder).strip()) else prev_col1\n",
    "        elif not (pd.isna(col1_holder) or not str(col1_holder).strip()):\n",
    "            col1 = col1_holder\n",
    "    else:\n",
    "        col1_holder = col1\n",
    "    \n",
    "    # Check for \"ALL OTHER\" in col1\n",
    "    if isinstance(col1, str):\n",
    "        col1_stripped_upper = col1.strip().upper()\n",
    "        is_all_other = col1_stripped_upper.startswith(\"ALL OTHER\")\n",
    "    else:\n",
    "        col1_stripped_upper = ''\n",
    "        is_all_other = False\n",
    "    \n",
    "    # Handle vicinity\n",
    "    null_vicinity = pd.isna(vicinity) or not str(vicinity).strip()\n",
    "    if null_vicinity:\n",
    "        if continuation:\n",
    "            if not (pd.isna(prev_col1) and pd.isna(col1)) and prev_col1 != col1:\n",
    "                vicinity_holder = vicinity\n",
    "            else:\n",
    "                vicinity = vicinity_holder if not (pd.isna(vicinity_holder) or not str(vicinity_holder).strip()) else prev_vicinity\n",
    "        elif not (pd.isna(vicinity_holder) or not str(vicinity_holder).strip()):\n",
    "            if not (pd.isna(prev_col1) and pd.isna(col1)) and prev_col1 != col1:\n",
    "                vicinity_holder = vicinity\n",
    "            else:\n",
    "                vicinity = vicinity_holder\n",
    "    else:\n",
    "        vicinity_holder = vicinity\n",
    "    \n",
    "    # 'ALL OTHER' logic\n",
    "    if is_all_other:\n",
    "        if not null_vicinity:\n",
    "            all_other_vicinity = vicinity\n",
    "        if all_other_vicinity:\n",
    "            vicinity = all_other_vicinity\n",
    "        else:\n",
    "            vicinity = ''\n",
    "            if debug:\n",
    "                print(f\"'col1' starts with 'ALL OTHER'. Setting 'vicinity' to blank.\")\n",
    "    else:\n",
    "        all_other_vicinity = None\n",
    "    \n",
    "    # Check for dash strings\n",
    "    def is_dash_string(var):\n",
    "        return isinstance(var, str) and re.fullmatch(r\"\\-+\", var) is not None\n",
    "    \n",
    "    matches = sum(is_dash_string(var) for var in [col1, vicinity, classification, zv])\n",
    "    if matches >= 3:\n",
    "        # Skip this row and increase age\n",
    "        return process_data_rows_recursive(\n",
    "            df, result_df, index + 1, final_index, \n",
    "            current_province, current_city, current_barangay, \n",
    "            header_indices, \n",
    "            prev_col1, prev_vicinity, prev_classification, prev_zvsqm, \n",
    "            continuation, \n",
    "            debug, debug_location, debug_header, \n",
    "            age + 1, col1_holder, vicinity_holder, all_other_vicinity\n",
    "        )\n",
    "    \n",
    "    # Append to result DataFrame\n",
    "    result_df.loc[len(result_df)] = [\n",
    "        current_province,\n",
    "        current_city,\n",
    "        current_barangay,\n",
    "        clean_value(col1, feature=True),\n",
    "        clean_value(vicinity, feature=True),\n",
    "        clean_value(classification, feature=True),\n",
    "        clean_value(zv, feature=True)\n",
    "    ]\n",
    "    \n",
    "    # Update previous values\n",
    "    new_prev_col1 = col1\n",
    "    new_prev_vicinity = vicinity\n",
    "    new_prev_classification = classification\n",
    "    new_prev_zvsqm = zv\n",
    "    \n",
    "    if debug:\n",
    "        print(result_df.loc[len(result_df)-1])\n",
    "        print(\"\\n-------\\n\")\n",
    "    \n",
    "    # Continue with next row (age reset to 0)\n",
    "    return process_data_rows_recursive(\n",
    "        df, result_df, index + 1, final_index, \n",
    "        current_province, current_city, current_barangay, \n",
    "        header_indices, \n",
    "        new_prev_col1, new_prev_vicinity, new_prev_classification, new_prev_zvsqm, \n",
    "        continuation, \n",
    "        debug, debug_location, debug_header, \n",
    "        0, col1_holder, vicinity_holder, all_other_vicinity\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main(df, debug=False, start=0, end=555, debug_location=False, debug_header=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_recursive(df, debug=False, start=0, end=555, debug_location=False, debug_header=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Updated Scraper",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
